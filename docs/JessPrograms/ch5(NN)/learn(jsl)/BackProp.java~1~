
import java.awt.*;
import java.util.*;
import java.lang.Math;
import java.io.* ; 

// standard backward propagation with momentum 
public class BackProp extends Object {
    String name ; 
    
  // data parameters 
    static DataSet ds ; 
    Vector data ;       // train/test data from file
    int recInx=0 ;      // current record index
    int numRecs=0 ;     // number of records in data
    int fieldsPerRec=0; 
    
  // error measures
    float sumSquaredError ;  // total SSE for an epoch 
    float aveRMSError ;      // average root-mean-square error
    int numPasses ;          // number of passes over the data set
    
  // network architecture parameters  
    int numInputs ;
    int numHid1 ;
    int numOutputs ; 
    int numUnits ;
    int numWeights ;
    
  // network control parameters
    int mode; 
    float learnRate ;
    float momentum; 
    float tolerance; 
    
  // network data   
    float activations[] ;
    float weights[] ;
    float wDerivs[] ;
    float thresholds[];
    float tDerivs[] ; 
    float tDeltas[] ;
    float teach[];   // target output values
    float error[]; 
    float deltas[];  // the error deltas
    float wDeltas[] ;
    
    TextArea textArea1 ; 

//BP1<-LA6
  BackProp(String Name) {
     name = Name ;
     data = new Vector() ; 
  }
  
     
    public void show_array(String name, float[] arr) {
        textArea1.appendText("\n" + name  + "= ") ; 
     for (int i=0 ; i < arr.length ; i++) {
        textArea1.appendText(arr[i] + "  ") ; 
     }
      
    }
    
    public void display_network() {
//      show_array("weights",weights);
//      show_array("thresholds", thresholds); 
	    show_array("activations",activations) ; 	  
//	    show_array("teach",teach) ;          
   
        String desired = ds.getClassFieldValue(recInx-1); 
        String actual = ds.getClassFieldValue(activations, numInputs+numHid1);
        textArea1.appendText("\n Desired: " + desired + " Actual: " + actual ) ; 
    }
    
//BP11->BP10 
    public float logistic(double sum) {
       return (float)(1.0 / (1 + Math.exp(-1.0 * sum)));      
    }

    // move data from train/test set into network input units
//BP7<-BP6
    public void readInputs() { 
     recInx = recInx % numRecs ; // keep index from 0 to n-1 records:0,1,2
     int inx=0 ;
     float[] tempRec = (float[])data.elementAt(recInx) ; // get record
     for (inx=0 ; inx < numInputs ; inx++ ) {
       activations[inx] = tempRec[inx] ; //activations[0]=0,activations[1]=0, ...   
     }
     for (int i=0 ; i < numOutputs; i++ ) {
       teach[i] = tempRec[inx++] ;   //teach[0]=0
     }
     recInx++ ; 
    }
    
    // do a single forward pass through the network 
//BP9<-BP8
    public void computeOutputs() {
     int i, j ;
     int firstHid1 = numInputs ;
     int firstOut = numInputs + numHid1 ; 
        
     // first layer
     int inx = 0 ;
     for(i = firstHid1 ; i < firstOut ; i++) {
       float sum = thresholds[i]; 
       for (j = 0 ; j < numInputs ; j++) { // compute net inputs
         sum += activations[j] * weights[inx++] ;
       }
//BP10->BP11:logistic(sum)
       activations[i] = logistic(sum) ;  // compute activation
     }  
     // second layer 
     for(i = firstOut ; i < numUnits ; i++) {
        float sum = thresholds[i] ;      
       for (j = firstHid1 ; j < firstOut ; j++) { // compute net inputs
         sum += activations[j] * weights[inx++] ;
       }
       activations[i] = logistic(sum) ;  // compute activation
     }         
    }

    // compute the errors using backward error propagation
    // weight changes get placed in (or added to) wDerivs
    // threshold changes get placed in (or added to) tDerivs
//BP13<-BP12
    public void computeError() {     
     int i, j ;
     int firstHid1 = numInputs ;
     int firstOut = numInputs + numHid1 ; 

     // clear hidden unit errors
     for(i = numInputs ; i < numUnits ; i++) {
        error[i] = (float)0.0 ; 
     }
     // compute output layer errors and deltas
     for(i = firstOut ; i < numUnits ; i++) {
        error[i] = teach[i-firstOut] - activations[i] ; // compute output errors  
        sumSquaredError += error[i] * error[i] ;       // accumulate squared errors
        if (Math.abs(error[i]) < tolerance) error[i] = (float)0.0 ; // close enough
        deltas[i] = error[i] * activations[i] * (1 - activations[i]);//error signal
     }//식5.5
    
     // compute hidden layer errors      
     int winx = numInputs * numHid1 ;    // offset into weight array 
     for(i = firstOut ; i < numUnits ; i++) {
        for (j = firstHid1; j < firstOut ; j++) {
            wDerivs[winx] += deltas[i] * activations[j] ;
                    //식5.4-은닉층의 연결강도 변화량:learnrate는 adjustWeights()에서 계산
            error[j] += weights[winx] * deltas[i] ;// 은닉층 오차값
            winx++ ;
        }
        tDerivs[i] += deltas[i] ; //thresholds의 변화량       
     }
         
     // compute hidden layer deltas
     for (i = firstHid1 ; i < firstOut ; i++) {
       deltas[i] = error[i] * activations[i] * (1 - activations[i]);      
     }//식5.6
     
     // compute input layer errors
     winx = 0 ;               // offset into weight array
     for(i = firstHid1 ; i < firstOut ; i++) {
        for (j = 0; j < firstHid1 ; j++) {
            wDerivs[winx] += deltas[i] * activations[j] ;
            error[j] += weights[winx] * deltas[i] ;
            winx++ ;
        }
        tDerivs[i] += deltas[i] ;        
     }  
     
    }
    
      
    // apply the changes to the weights
////BP15<-BP14
    public void adjustWeights() {
     int i ;

     // first walk through the weights array
     for(i = 0; i < weights.length ; i++) {
        wDeltas[i] = (learnRate * wDerivs[i]) + (momentum * wDeltas[i]);
        weights[i] += wDeltas[i] ;   // modify the weight 
        wDerivs[i] = (float)0.0 ; 
     }      
     
     // then walk through the threshold array
     for(i=numInputs ; i < numUnits ; i++) {
        tDeltas[i] = learnRate * tDerivs[i]  + (momentum * tDeltas[i]);
        thresholds[i] += tDeltas[i] ; // modify the threshold 
        tDerivs[i] = (float)0.0 ;
     }
     // if at the end of an epoch, compute average RMS Error
     if (recInx == numRecs) {     
        numPasses++ ;  // increment pass counter
        aveRMSError = (float)Math.sqrt( sumSquaredError / (numRecs * numOutputs));  
        sumSquaredError = (float)0.0 ;  // clear the accumulator        
     }
    }

//BP5<-LA9
    public void process() {

//BP6->BP7
     readInputs() ;   // set input unit activations
//BP8->BP9     
     computeOutputs() ;  // do forward pass through network
//BP12->BP13      
     computeError() ;  // compute error and deltas
//BP14->BP15: adjustWeights()
     // only adjust if in training mode
     if (mode == 0) adjustWeights() ;   // apply changes to weights
           
    }

//BP4<-BP3
    public void reset() {
        int i ;
        for (i=0 ; i < weights.length ; i++) {
          weights[i] = (float)1.0 - (float)Math.random();    // between 0 and +1 
          wDeltas[i] = (float)0.0 ;
          wDerivs[i] = (float)0.0 ; 
        }
        for (i=0 ; i < numUnits ; i++) {
          thresholds[i] = (float)1.0 - (float)Math.random();    // between 0 and +1 
          tDeltas[i] = (float)0.0 ;
          tDerivs[i] = (float)0.0 ; 
        } 
    }
    
//BP2<-LA8
    // create a Back Prop network with specified architecture
    public void createNetwork(int NumIn, int NumHid1, int NumOut){
        
        // set the network architecture 
        numInputs = NumIn ;  //2       
        numHid1 = NumHid1;   //2       
        numOutputs = NumOut; //1       
        numUnits = numInputs + numHid1 + numOutputs ;//5
        numWeights = (numInputs*numHid1) + (numHid1*numOutputs);//6
        
        // initialize control parameters
        learnRate = (float)0.2;
        momentum = (float)0.7 ;      
        tolerance = (float)0.1 ; 
        mode = 0 ;             // 0 = train mode, 1 = run mode
        aveRMSError = (float)0.0 ; 
        numPasses = 0; 
        
        // create weight and error arrays
        activations = new float[numUnits]; // unit activations       
        weights = new float[numWeights];   
        wDerivs = new float[numWeights];   // accumulated wDeltas
        wDeltas = new float[numWeights];   // weight changes  
        thresholds = new float[numUnits];  
        tDerivs = new float[numUnits];     // accumulated tDeltas
        tDeltas = new float[numUnits] ;    // threshold changes
        teach = new float[numOutputs] ;    // desired outputs    
        deltas = new float[numUnits];    
        error = new float[numUnits] ; 
//BP3->BP4
        reset() ;  // reset and initialize the weight arrays
   
       return;  
    }


	

 
}
